---
name: Testing conversation flows
title: Testing conversations
route: /docs/rasa/nlu/testing/
menu: Authoring conversations
meta:
  - name: description
    content: Add test sequences for conversational flows.
  - name: keywords
    content: rasa ui frontend test
permalink: /rasa/:slug
---

# Overview

Once a conversation flow has been created it is important to ensure it continues to work. Botfront conversation testing is designed to help with this.

A test story is a language dependant sequence of user events and bot responses created from a conversation.

Test stories can be run, an action which sends the test to the custom BotRegressionTesting channel in rasa for botfront. This channel simulates a user entering the text of each user event into the chatbot and then verifies that the parsed intents and entities, as well as the generated bot response names match the parsed data and bot response names currently in the test.
When a test run is complete you will get a notification with the number of tests that passed and failed during the test run.

## Creating Tests

You can create a new test two different ways.

**Creating via the webchat panel**

Clicking on the clipboard icon in the webchat panel will create a new test from the current conversation.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/create_test_from_webchat.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

**Creating via the conversations screen**

In Incoming -> Conversations you can create a new test by clicking on the clipboard icon in the action bar above the conversation.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/create_test_from_conversation.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

Conversations from versions before 1.0 are not guaranteed to be supported.

## Running Tests

You can trigger a test run that will run all the tests in your project via the training button's dropdown menu.
In the same menu there is an option to run all the tests that use the currently selected language.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/run_all_tests.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

You can also run tests individually by selecting them in the dialogue menu and clicking on the play button in the top left corner of the test viewer.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/run_one_test.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

When your test run is complete a notification will appear in the top right corner of the window with the number of passing and failing tests.
All tests that failed in the most recent test run can be found in the **Failing tests** smart group.
This smart group ignores the by-language filter normally applied to tests to make it easier to update failing tests.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/run_failing_tests.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

## Updating failing tests

When a test fails you will see a diff with the expected events appearing in red and the actual events as parsed and generated by rasa appearing in green.
You may overwrite the test with the newly parsed results by clicking on **set actual as expected** in the topbar of the test. This will delete all the events in green and save the events in red as part of the test.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/update_failing_test.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

If a significant change to the conversation's flow has been made you will need to delete your test and create a new one via the steps described earlier in this section.
