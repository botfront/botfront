---
name: Testing conversation flows
title: Testing conversations
route: /docs/rasa/nlu/testing/
menu: Authoring conversations
meta:
  - name: description
    content: Add test sequences for conversational flows.
  - name: keywords
    content: rasa ui frontend test
permalink: /rasa/:slug
---

# Test sequence stories

Once a conversational flow has been created it is important to ensure it continues to work. Botfront conversational testing is designed to help with this.

A test story is a sequence of user and bot events that represents a full conversation between a user and a chatbot. 

The custom BotRegressionTesting channel handles conversational test runs. It simulates a user entering each user event into the chatbot and verrifies that the parse data and resulting bot responses match the current expected.
Note that only the bot response name is validated, not it's content.

Tests are language specific as the user events are parsed by rasa. In the dialogue menu only the test in the current language are displayed. An exception to this is made for failing tests which are always displayed in the Failing Tests smart group.

## Creating Tests

You can create a new test two different ways.

**Creating via the webchat pannel**

In the webchat pannel clicking on the clipboard icon will create a new test from the current conversation.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/create_test_from_webchat.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

**Creating via the conversations screen**

In Incoming -> Conversations you can create a new test by clicking on the clipboard icon in the action bar above the conversation.

<video autoplay muted loop width="100%" controls>
  <source src="../../../videos/tests/create_test_from_conversation.m4v" type="video/mp4"/>
  Your browser does not support the video tag.
</video>

conversations from versions before 1.0 are not garuanteed to be supported.

## Running Tests

You can trigger a test run that will run all the tests in your project via the training button's dropdown menu.

In the same menu there is an option to run all the tests that use the currently selected language.

You can also run tests individually by selecting them in the dialogue menu and clicking on the play button in the top left corner of the test viewer.

When your test run is complete a notification will appear in the top right corner of the window with the number of passing and failing tests.

All tests that failed in the most recent test run can be found in the **Failing tests** smart group.
This smart group ignores the by-language filter normally applied to tests to make it easier to update failing tests.

## Updating failing tests

When a test fails you will see a diff with the expected events appearing in red and the actual events as parsed and generated by rasa appearing in green.

You may overwrite the test with the newly parsed results by clicking on **set actual as expected** in the topbar of the test. This will delete all the events in green and save the events in red as part of the test.

If a significant change to the conversation's flow has been made you will need to delete the old test and create a new one via the steps descirbed earlier in this section.
